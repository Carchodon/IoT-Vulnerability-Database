import mysql.connector
from database import db
import pandas as pd

##imports de reconocimineto
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
import re
import string

class mod:
    def get_dataframe_classified(self,tableName):
        database = db()
        db_cursor = database.conectar("tfm")
        sql_statement = "SELECT * FROM " +  tableName
        db_cursor.execute(sql_statement)
        output = db_cursor.fetchall()

        ##creamos dataframe vacio
        df=pd.DataFrame(columns =['id','score','description', 'part', 'vendor', 'devType'])

        #####EDITAMOS LAS DESCRIPCIONES######
        iterator=0
        iot=0
        for x in output:
            id = x[0]
            score = x[1]
            description=x[2]
            part = x[3]
            vendor = x[4]
            deviceType = x[5]
            
            ##transformar a minusculas
            text = description.lower()

            ##quitar puntuacion
            text = "".join([char for char in text if char not in string.punctuation])
            
            ##Aplicar reglas
            rules = [
                    {r'>\s+': u'>'},  # remove spaces after a tag opens or closes
                    {r'\s+': u' '},  # replace consecutive spaces
                    {r'\s*<br\s*/?>\s*': u'\n'},  # newline after a <br>
                    {r'</(div)\s*>\s*': u'\n'},  # newline after </p> and </div> and <h1/>...
                    {r'</(p|h\d)\s*>\s*': u'\n\n'},  # newline after </p> and </div> and <h1/>...
                    {r'<head>.*<\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>
                    {r'<a\s+href="([^"]+)"[^>]*>.*</a>': r'\1'},  # show links instead of texts
                    {r'[ \t]*<[^<]*?/?>': u''},  # remove remaining tags
                    {r'^\s+': u''}  # remove spaces at the beginning
                ]
            for rule in rules:
                for (k, v) in rule.items():
                    regex = re.compile(k)
                    text = regex.sub(v, text)
                text = text.rstrip()

            ##Tokenization y quitamos los Stop words
            stop_words = set(stopwords.words('english'))
            word_tokens = word_tokenize(text)
            filtered_sentence = [w for w in word_tokens if not w in stop_words]
            filtered_sentence = []
            for w in word_tokens:
                if w not in stop_words:
                    filtered_sentence.append(w)

            ##Stemming
            porter=PorterStemmer()
            stemmed = [porter.stem(word) for word in filtered_sentence]
            finalDescription=stemmed

            ##Creamos un string a partir de los tokens
            descriptionAsString = finalDescription[0]
            for i in range(1,len(finalDescription)):
                descriptionAsString=descriptionAsString+" "+finalDescription[i]

            ##AÃ±adimos una nueva fila al dataframe
            df.loc[iterator]=[id,score,descriptionAsString,part,vendor,deviceType]
            
            iterator+=1
        
        return df
    
    def get_dataframe_not_classified(self,tableName):
        database = db()
        db_cursor = database.conectar("tfm")
        sql_statement = "SELECT * FROM " +  tableName
        db_cursor.execute(sql_statement)
        output = db_cursor.fetchall()

        ##creamos dataframe vacio##
        df=pd.DataFrame(columns =['id','score','description', 'part', 'vendor'])

        ######EDITAMOS LAS DESCRIPCIONES######
        iterator=0
        iot=0
        for x in output:
            id = x[0]
            score = x[1]
            description=x[2]
            part = x[3]
            vendor = x[4]
            text = description.lower()
            text = "".join([char for char in text if char not in string.punctuation])
            ##Clean the text
            rules = [
                    {r'>\s+': u'>'},  # remove spaces after a tag opens or closes
                    {r'\s+': u' '},  # replace consecutive spaces
                    {r'\s*<br\s*/?>\s*': u'\n'},  # newline after a <br>
                    {r'</(div)\s*>\s*': u'\n'},  # newline after </p> and </div> and <h1/>...
                    {r'</(p|h\d)\s*>\s*': u'\n\n'},  # newline after </p> and </div> and <h1/>...
                    {r'<head>.*<\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>
                    {r'<a\s+href="([^"]+)"[^>]*>.*</a>': r'\1'},  # show links instead of texts
                    {r'[ \t]*<[^<]*?/?>': u''},  # remove remaining tags
                    {r'^\s+': u''}  # remove spaces at the beginning
                ]
            for rule in rules:
                for (k, v) in rule.items():
                    regex = re.compile(k)
                    text = regex.sub(v, text)
                text = text.rstrip()

            #print (text)

            ##Tokenization and Stop words
            stop_words = set(stopwords.words('english'))
            word_tokens = word_tokenize(text)
            filtered_sentence = [w for w in word_tokens if not w in stop_words]
            filtered_sentence = []
            for w in word_tokens:
                if w not in stop_words:
                    filtered_sentence.append(w)
            #print(filtered_sentence)
            ##stemming
            porter=PorterStemmer()
            stemmed = [porter.stem(word) for word in filtered_sentence]
            #print(stemmed)
            finalDescription=stemmed

            descriptionAsString = finalDescription[0]
            for i in range(1,len(finalDescription)):
                descriptionAsString=descriptionAsString+" "+finalDescription[i]


            df.loc[iterator]=[id,score,descriptionAsString,part,vendor]
            
            
            iterator+=1
        return df
